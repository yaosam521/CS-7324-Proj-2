{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37908c56-ae0f-45c9-b019-bc3864fd3e0d",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "by:\n",
    "- Rebecca Kuhlman\n",
    "- Michael Amberg\n",
    "- Sam Yao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b45028-38cc-4d4b-89f0-8c6bc361356b",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "Identifying the type of brain tumor a patient has is an important step in figuring out the treatment plan of a patient. They can be diagnosed via MRI imaging, leading to interest in using machine learning to diagnose the patient. Having a second opinion on brain tumor diagnoses would help improve patient care and outcomes, and lessen stress on doctors. A machine learning model could also speed up analysis time and pick out which patients are in need of urgent treatment.\n",
    "\n",
    "In this dataset, there is glioma, meningioma, and pituitary tumors, as well as MRI images with no tumors.\n",
    "Glioma tumors are usually malignant, while meningioma and pituitary tumors are usually benign. Different types of tumors are made of different types of cells and have a location where they are most likely to be located.\n",
    "More information can be found at: https://www.mayoclinic.org/diseases-conditions/brain-tumor/symptoms-causes/syc-20350084\n",
    "\n",
    "There are many other types of tumors that future algorithms will be need to address. The majority of other types of tumors are more common in children, while the set we are dealing with are all adult brain images.\n",
    "\n",
    "Because the model deals with health conditions that have extreme affects on the patient, model accuracy is extremely important. Furthermore, accuracy must fine-tuned to avoid fatal misdiagnosis. While incorrectly marking a patient with a benign tumor as malignant is wasteful, the adverse affects are minimal. Inversely, misdiagnosing a malignant tumor as benign may have fatal effects for the patient. Therefore, the designed model must minimize the rate of false negatives with accuracy of 95% or more.\n",
    "\n",
    "It should be noted that the majority of misdiagnose of brain tumors happen before a brain scan or related test is ordered.\n",
    "https://paulandperkins.com/brain-tumors/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d617b08d-342f-4b9f-bed7-7de11cbbcd11",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e608d-78c3-49eb-aa11-90f6ace85011",
   "metadata": {},
   "source": [
    "Several helpful sources that helped this part of the section include:\n",
    "- [1] https://pillow.readthedocs.io/en/stable/handbook/tutorial.html\n",
    "- [2] https://towardsdatascience.com/loading-custom-image-dataset-for-deep-learning-models-part-1-d64fa7aaeca6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a198ffb6-8712-4419-94d6-92a570e85347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image # Utilized Source [2]\n",
    "\n",
    "img = Image.open(\"./Training/glioma_tumor/gg (1).jpg\") # Utilized Source [1]\n",
    "img_arr = np.array(img)\n",
    "new_arr = list()\n",
    "for x in img_arr:\n",
    "    for y in x:\n",
    "        new_arr.append(y)\n",
    "print(len(new_arr))\n",
    "# This method creates the data, whether training or testing, in the form we desire\n",
    "# Uses code from source [2] to create the training datasets\n",
    "def create_dataset(img_folder):\n",
    "    # Read through all files in \"./Training\"\n",
    "    img_data_array=[]\n",
    "    class_name=[]\n",
    "    for dir1 in os.listdir(img_folder):\n",
    "        for file in os.listdir(os.path.join(img_folder, dir1)):\n",
    "            image_path= os.path.join(img_folder, dir1,  file)\n",
    "            image= np.array(Image.open(image_path))\n",
    "            image = np.resize(image, (1,262144,3)) #Vectorizes each image\n",
    "            image = image.astype('float32')\n",
    "            #image /= 255  \n",
    "            img_data_array.append(image)\n",
    "            class_name.append(dir1)\n",
    "    # return array with training data.\n",
    "    return img_data_array, class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "092cc69c-f409-4eb0-9034-72751c377ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training, training_classes = create_dataset(\"./Training\")\n",
    "df_testing, testing_classes = create_dataset(\"./Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1f7e97c-0842-4b60-97ba-74a0ad069a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 262144, 3)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Reduction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2870\n",
      "n_features: 1\n",
      "n_classes: 4\n",
      "Original Image Sizes 512 by 3\n"
     ]
    }
   ],
   "source": [
    "# get some of the specifics of the dataset\n",
    "dfTrain = pd.DataFrame.from_records(df_training, training_classes)\n",
    "X = dfTrain\n",
    "y = training_classes\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "_, h, w = img_arr.shape\n",
    "n_classes = 4\n",
    "\n",
    "print(\"n_samples: {}\".format(n_samples))\n",
    "print(\"n_features: {}\".format(n_features))\n",
    "print(\"n_classes: {}\".format(n_classes))\n",
    "print(\"Original Image Sizes {} by {}\".format(h,w))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lets do some PCA of the features and go from 1850 features to 20 features\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 300\n",
    "print (\"Extracting the top %d eigenfaces from %d faces\" % (\n",
    "    n_components, X.shape[0]))\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "%time pca.fit(X.copy())\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reconstruct_image(trans_obj,org_features):\n",
    "    low_rep = trans_obj.transform(org_features)\n",
    "    rec_image = trans_obj.inverse_transform(low_rep)\n",
    "    return low_rep, rec_image\n",
    "\n",
    "idx_to_reconstruct = 1\n",
    "X_idx = X[idx_to_reconstruct]\n",
    "low_dimensional_representation, reconstructed_image = reconstruct_image(pca,X_idx.reshape(1, -1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " randomized principle components analysis. Visualize the explained variance of each component. Analyze how many dimensions are required to adequately represent your image data. Explain your analysis and conclusion."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print (\"Extracting the top %d eigenfaces from %d faces\" % (\n",
    "    n_components, X.shape[0]))\n",
    "\n",
    "rpca = PCA(n_components=n_components, svd_solver='randomized')\n",
    "%time rpca.fit(X.copy())\n",
    "eigenfaces = rpca.components_.reshape((n_components, h, w))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compare the representation using PCA and Randomized PCA. The method you choose to compare dimensionality methods should quantitatively explain which method is better at representing the images with fewer components.  Do you prefer one method over another? Why?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "2cc100fd-a872-4c07-a457-b645624831e7",
   "metadata": {},
   "source": [
    "feature extraction upon the images using DAISY. Try different parameters for your image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f380f8-8286-4240-a944-d24b84df2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import daisy\n",
    "\n",
    "# lets first visualize what the daisy descriptor looks like\n",
    "features, img_desc = daisy(img,\n",
    "                           step=20,\n",
    "                           radius=20,\n",
    "                           rings=2,\n",
    "                           histograms=8,\n",
    "                           orientations=8,\n",
    "                           visualize=True)\n",
    "imshow(img_desc)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now let's understand how to use it\n",
    "features = daisy(img, step=20, radius=20, rings=2, histograms=8, orientations=4, visualize=False)\n",
    "print(features.shape)\n",
    "print(features.shape[0]*features.shape[1]*features.shape[2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a function to take in the row of the matrix and return a new feature\n",
    "def apply_daisy(row,shape):\n",
    "    feat = daisy(row.reshape(shape), step=20, radius=20,\n",
    "                 rings=2, histograms=8, orientations=4,\n",
    "                 visualize=False)\n",
    "    return feat.reshape((-1))\n",
    "\n",
    "%time test_feature = apply_daisy(X[3],(h,w))\n",
    "test_feature.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "# find closest image to current image\n",
    "idx1 = 5\n",
    "distances = copy.deepcopy(dist_matrix[idx1,:])\n",
    "distances[idx1] = np.infty # dont pick the same image!\n",
    "idx2 = np.argmin(distances)\n",
    "\n",
    "plt.figure(figsize=(7,10))\n",
    "plt.subplot(1,2,1)\n",
    "imshow(X[idx1].reshape((h,w)))\n",
    "plt.title(\"Original Image\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "imshow(X[idx2].reshape((h,w)))\n",
    "plt.title(\"Closest Image\")\n",
    "plt.grid()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Does this feature extraction method show promise for your prediction task? Why?\n",
    "Use visualizations to analyze this questions. For example, use a heat map of the pairwise differences (ordered by class) among all extracted features. Another option, build a nearest neighbor classifier to see actual classification performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "b0d288f5-6294-4ffc-b5e1-b7b2f4bd6179",
   "metadata": {},
   "source": [
    "## Exceptional Work ðŸ˜¡"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Additional feature extraction techniques(Gabor filters, keypoint matching, ordered gradients) Several are provided in the notebooks and you might research techniques known in the computer vision literature.\n",
    "Does this feature extraction method show promise for your prediction task? Why?\n",
    "Use visualizations to analyze this questions. For example, use a heat map of the pairwise differences (ordered by class) among all extracted features. Another option, build a nearest neighbor classifier to see actual classification performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb467a8-0d45-4b50-9ecf-cc023520f0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
